{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-A2B-HITS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibSckvWI_32s",
        "outputId": "9d0304ef-eaa6-4081-905f-dfeabae7f2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (2.6.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "yyVyrINHCCYc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX4FS3UFo_Xu",
        "outputId": "eeebf76e-91b6-4498-ad3b-99e6914f97e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import linalg as LA\n",
        "import random\n",
        "import time\n",
        "import csv"
      ],
      "metadata": {
        "id": "nz34sKSjmsIt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HITS:\n",
        "  '''\n",
        "  Class for implementing the HITSalgorithm\n",
        "  Class variables:\n",
        "    query: list containing the word tokens in the query string\n",
        "    G: networkx graph (web graph)\n",
        "    adj: adjacency matrix of G\n",
        "    A: adjacency matrix of sub graph of G\n",
        "    root_set: root set of the query\n",
        "    base_set: base set of the query\n",
        "    base_edges: edges present in the sub graph of G (edges b/w nodes of base_set)\n",
        "    hub_score: list containing the hub scores of the nodes\n",
        "    auth_score: list containing the auth scores of the nodes\n",
        "  '''\n",
        "  def __init__(self, G):\n",
        "    '''Function for taking input and initializing variables'''\n",
        "\n",
        "    # inputting the query and converting it to lower case\n",
        "    self.query = input(\"Please enter the query: \").lower()\n",
        "    # Tokenizing the query into words and storing them as a list\n",
        "    self.query = word_tokenize(self.query)\n",
        "    print(\"Query: \", self.query, \"\\n\")\n",
        "\n",
        "    self.G = G\n",
        "    # l = number of nodes in the web graph\n",
        "    l = len(self.G.nodes)\n",
        "\n",
        "    # initilizing the adj matrix to be of size l*l and containig 0s\n",
        "    self.adj = np.zeros((l, l))\n",
        "\n",
        "    self.A = []\n",
        "\n",
        "    self.root_set = []\n",
        "    self.base_set = []\n",
        "\n",
        "    self.base_edges = []\n",
        "\n",
        "    self.hub_scores = []\n",
        "    self.auth_scores = []\n",
        "\n",
        "  def initialize_adj(self):\n",
        "    '''Function to change the adjacent matrix according to the edges in G'''\n",
        "    for edge in self.G.edges:\n",
        "      s = edge[0]\n",
        "      d = edge[1]\n",
        "      self.adj[s][d] = 1\n",
        "\n",
        "  def find_root(self):\n",
        "    '''Function to find the root set'''\n",
        "    for i in range(0, len(self.G.nodes)):\n",
        "      flag = True\n",
        "      for word in self.query:\n",
        "        if word not in self.G.nodes[i]['page_content'].lower():\n",
        "          flag = False\n",
        "          break\n",
        "      if flag == True:\n",
        "        self.root_set.append(i)\n",
        "\n",
        "    print(\"ROOT SET: \", self.root_set, \"\\n\")\n",
        "\n",
        "  def find_base(self):\n",
        "    '''Function to find the base set'''\n",
        "    \n",
        "    # adding the nodes in the root_set to the base_set\n",
        "    for node in self.root_set:\n",
        "      self.base_set.append(node)\n",
        "    \n",
        "    # adding nodes connected to the root_set nodes into the base set\n",
        "    for edge in self.G.edges:\n",
        "      src = edge[0]\n",
        "      dest = edge[1]\n",
        "      if src in self.root_set:\n",
        "        if dest not in self.base_set:\n",
        "          self.base_set.append(dest)\n",
        "      elif dest in self.root_set:\n",
        "        if src not in self.base_set:\n",
        "            self.base_set.append(src)\n",
        "    \n",
        "    print(\"BASE SET: \", self.base_set, \"\\n\")\n",
        "    \n",
        "    # initializing adjacency matrix of sub graph to be of size l*l (l=size(base_set)) and to contain all 0s initially\n",
        "    l = len(self.base_set)\n",
        "    self.A = np.zeros((l,l))\n",
        "    \n",
        "    for i in self.base_set:\n",
        "      for j in self.base_set:\n",
        "        if self.adj[i][j] == 1:\n",
        "          s = self.base_set.index(i)\n",
        "          d = self.base_set.index(j)\n",
        "          self.A[s][d] = 1\n",
        "          # storing the edges present in subgraph in base_edges\n",
        "          self.base_edges.append((i,j))\n",
        "\n",
        "  def find_hub_score(self):\n",
        "    '''Function to find the hub scores of the nodes'''\n",
        "\n",
        "    aaT = np.dot(self.A, self.A.T)\n",
        "    \n",
        "    # finding the eigenvector of (A.Atranspose) using the linear alg library\n",
        "    v, V= LA.eig(a=np.array(aaT),b=None,left=True,right=False,overwrite_a=False,overwrite_b=False,check_finite=False)\n",
        "\n",
        "    # v, V = LA.eig(aaT)\n",
        "    # self.hub_scores = V[:, 0].T\n",
        "\n",
        "    # hub_scores = principle left eigen vector of (A.Atranspose)\n",
        "    max_eig_val = 0\n",
        "    max_eig_val_ind = 0\n",
        "    for i in range(len(v)):\n",
        "      if (v[max_eig_val_ind] < v[i]):\n",
        "        max_eig_val_ind = i\n",
        "        max_eig_val = v[i]\n",
        "\n",
        "    self.hub_scores = V[:, max_eig_val_ind]\n",
        "    self.hub_scores = self.hub_scores/LA.norm(self.hub_scores)\n",
        "\n",
        "    # normalizing the hub scores\n",
        "    self.hub_scores = self.hub_scores/sum(self.hub_scores)\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(self.base_set)):\n",
        "      result.append((self.base_set[i], self.hub_scores[i]))\n",
        "    result=sorted(result,key=itemgetter(1),reverse=True)\n",
        "    print(\"Page Rankings:\\nPage No\\tHub score\")\n",
        "    index=1\n",
        "    for x in result:\n",
        "      # if index == 10:\n",
        "      #   break\n",
        "      for j in x:\n",
        "        print(j,end=\"\\t\")\n",
        "      index=index+1\n",
        "      print(\"\\n\")\n",
        "\n",
        "    # print(\"HUB SCORES: \")\n",
        "    # for i in range(len(self.base_set)):\n",
        "    #   print(self.base_set[i], \":\", self.hub_scores[i])\n",
        "    # print(\"\\nSum of Hub scores: \", sum(self.hub_scores), \"\\n\")\n",
        "\n",
        "  def find_auth_score(self):\n",
        "    '''Function to find the auth scores of the nodes'''\n",
        "    aTa = np.dot(self.A.T, self.A)\n",
        "    # finding the eigenvector of (Atranspose.A) using the linear alg library\n",
        "    # v, V = LA.eig(aTa)\n",
        "    # self.auth_scores = V[:, 0].T\n",
        "    v, V= LA.eig(a=np.array(aTa),b=None,left=True,right=False,overwrite_a=False,overwrite_b=False,check_finite=False)\n",
        "\n",
        "    # auth_scores = principle left eigen vector of (Atranspose.A)\n",
        "    max_eig_val = 0\n",
        "    max_eig_val_ind = 0\n",
        "    for i in range(len(v)):\n",
        "      if (v[max_eig_val_ind] < v[i]):\n",
        "        max_eig_val_ind = i\n",
        "        max_eig_val = v[i]\n",
        "\n",
        "    self.auth_scores = V[:, max_eig_val_ind]\n",
        "    self.auth_scores = self.auth_scores/LA.norm(self.auth_scores)\n",
        "\n",
        "    # normalizing the auth scores\n",
        "    self.auth_scores = self.auth_scores/sum(self.auth_scores)\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(self.base_set)):\n",
        "      result.append((self.base_set[i], self.auth_scores[i]))\n",
        "    result=sorted(result,key=itemgetter(1),reverse=True)\n",
        "    print(\"Page Rankings:\\nPage No\\tAuth score\")\n",
        "    index=1\n",
        "    for x in result:\n",
        "      # if index == 10:\n",
        "        # break\n",
        "      for j in x:\n",
        "        print(j,end=\"\\t\")\n",
        "      index=index+1\n",
        "      print(\"\\n\")\n",
        "\n",
        "    # print(\"AUTH SCORES: \")\n",
        "    # for i in range(len(self.base_set)):\n",
        "    #   print(self.base_set[i], \":\", self.auth_scores[i])\n",
        "    # print(\"\\nSum of Auth scores: \", sum(self.auth_scores), \"\\n\")\n",
        "\n",
        "  def validating(self):\n",
        "    '''Function to compare results obtained from \n",
        "    find_auth_score and find_hub_score with results obtained by using the library function\n",
        "    '''\n",
        "    h, a = nx.hits(self.G.subgraph(self.base_set))\n",
        "    print(\"HUB SCORES: \")\n",
        "    for i in range(len(self.base_set)):\n",
        "      print(self.base_set[i], \": \", self.hub_scores[i], \"  \", h[self.base_set[i]])\n",
        "    # print(h)\n",
        "    print(\"\\nAUTH SCORES: \")\n",
        "    for i in range(len(self.base_set)):\n",
        "      print(self.base_set[i], \": \", self.auth_scores[i], \"  \", a[self.base_set[i]])\n",
        "    # print(a)\n",
        "    \n",
        "  def run(self):\n",
        "    '''Function to run the algorithm'''\n",
        "    self.initialize_adj()\n",
        "    self.find_root()\n",
        "    self.find_base()\n",
        "    if (len(self.base_set) == 0):\n",
        "      print(\"Base set is empty\\n\")\n",
        "      return\n",
        "    self.find_hub_score()\n",
        "    self.find_auth_score()\n",
        "\n",
        "    # print(\"\\nValidating...\")\n",
        "    # self.validating()\n",
        "\n",
        "# ###########     ANALYSIS     #####################################################\n",
        "  def find_hub_score_analysis(self):\n",
        "    aaT = np.dot(self.A, self.A.T)\n",
        "    v, V= LA.eig(a=np.array(aaT),b=None,left=True,right=False,overwrite_a=False,overwrite_b=False,check_finite=False)\n",
        "    max_eig_val = 0\n",
        "    max_eig_val_ind = 0\n",
        "    for i in range(len(v)):\n",
        "      if (v[max_eig_val_ind] < v[i]):\n",
        "        max_eig_val_ind = i\n",
        "        max_eig_val = v[i]\n",
        "    self.hub_scores = V[:, max_eig_val_ind]\n",
        "    self.hub_scores = self.hub_scores/LA.norm(self.hub_scores)\n",
        "    self.hub_scores = self.hub_scores/sum(self.hub_scores)\n",
        "\n",
        "  def find_auth_score_analysis(self):\n",
        "    aTa = np.dot(self.A.T, self.A)\n",
        "    v, V= LA.eig(a=np.array(aTa),b=None,left=True,right=False,overwrite_a=False,overwrite_b=False,check_finite=False)\n",
        "    max_eig_val = 0\n",
        "    max_eig_val_ind = 0\n",
        "    for i in range(len(v)):\n",
        "      if (v[max_eig_val_ind] < v[i]):\n",
        "        max_eig_val_ind = i\n",
        "        max_eig_val = v[i]\n",
        "    self.auth_scores = V[:, max_eig_val_ind]\n",
        "    self.auth_scores = self.auth_scores/LA.norm(self.auth_scores)\n",
        "    self.auth_scores = self.auth_scores/sum(self.auth_scores)\n",
        "\n",
        "  def analysis(self):\n",
        "    csvfile = open(\"HITS_Analysis.csv\", \"w\")\n",
        "    fields = [\"Number of edges\", \"Runtime\"]\n",
        "    rows = []\n",
        "    n = len(self.G.nodes)\n",
        "    for i in range(50):\n",
        "      start = time.time()\n",
        "      b = random.randint(1,n)\n",
        "      self.base_set = []\n",
        "      for j in range(1,b):\n",
        "        a = random.randint(1,n)\n",
        "        self.base_set.append(a)\n",
        "      self.find_hub_score_analysis()\n",
        "      self.find_auth_score_analysis()\n",
        "      end = time.time()\n",
        "      runtime = end-start\n",
        "      edges = len(self.G.subgraph(self.base_set).edges)\n",
        "      rows.append([edges, runtime])\n",
        "    rows.sort()\n",
        "    csvwriter = csv.writer(csvfile) \n",
        "    # writing the fields \n",
        "    csvwriter.writerow(fields) \n",
        "    # writing the data rows \n",
        "    csvwriter.writerows(rows)\n",
        "    csvfile.close()\n"
      ],
      "metadata": {
        "id": "5w-i15L9_l9i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def driver():\n",
        "  '''driver code'''\n",
        "  # web_graph_file = '/content/drive/MyDrive/4-2/web_graph.gpickle'\n",
        "  web_graph_file = '/content/web_graph.gpickle'\n",
        "  G = nx.read_gpickle(web_graph_file)\n",
        "  hits = HITS(G)\n",
        "  hits.run()\n",
        "  # hits.analysis()"
      ],
      "metadata": {
        "id": "WR5B8EqAEt7a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb61R0u-FRBV",
        "outputId": "61619767-2c7c-4dc7-fb98-9a778304c9d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the query: gunfire\n",
            "Query:  ['gunfire'] \n",
            "\n",
            "ROOT SET:  [77] \n",
            "\n",
            "BASE SET:  [77, 15, 66, 8] \n",
            "\n",
            "Page Rankings:\n",
            "Page No\tHub score\n",
            "15\t0.3845864380372763\t\n",
            "\n",
            "77\t0.3660254037844386\t\n",
            "\n",
            "8\t0.16489956693907262\t\n",
            "\n",
            "66\t0.08448859123921251\t\n",
            "\n",
            "Page Rankings:\n",
            "Page No\tAuth score\n",
            "66\t0.3660254037844387\t\n",
            "\n",
            "8\t0.30009784679806367\t\n",
            "\n",
            "77\t0.1875382167312628\t\n",
            "\n",
            "15\t0.14633853268623478\t\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for generating documentation"
      ],
      "metadata": {
        "id": "I_haSpklA1dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pdoc3"
      ],
      "metadata": {
        "id": "SCc5_eKQZF0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file1 = \"/content/ir_a2b_hits-2.py\"\n",
        "\n",
        "# from pdoc import text\n",
        "# # import file\n",
        "# doc = open(\"HITS_doc.md\", \"w\")\n",
        "# doc.write(text(file1))\n",
        "# # finding_all_unique_words_and_freq\n",
        "# doc.close()"
      ],
      "metadata": {
        "id": "tTy5OilsZK7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file1 = \"/content/ir_a2a_pagerank.py\"\n",
        "\n",
        "# from pdoc import text\n",
        "# # import file\n",
        "# doc = open(\"2A-PageRank_doc.md\", \"w\")\n",
        "# doc.write(text(file1))\n",
        "# # finding_all_unique_words_and_freq\n",
        "# doc.close()"
      ],
      "metadata": {
        "id": "bCP0-Cy9du22"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}